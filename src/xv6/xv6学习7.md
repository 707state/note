# 调度

任何操作系统运行的进程数量都可能超过计算机的CPU数量，因此需要制定一个方案，在各进程之间分时共享CPU。理想情况下，这种共享对用户进程是透明的。一种常见的方法是通过将进程复用到硬件CPU上，给每个进程提供它有自己的虚拟CPU的假象。


## multiplexing

xv6通过在两种情况下将CPU从一个进程切换到另一个进程来实现复用。首先，xv6的sleep和wakeup机制会进行切换，这会发生在进程等待设备或管道I/O，或等待子进程退出，或在sleep系统调用中等待。其次，xv6周期性地强制切换，以应对长时间不进行sleep操作的计算进程。这种复用造成了每个进程都有自己的CPU的假象，就像xv6使用内存分配器和硬件页表造成每个进程都有自己的内存的假象一样。



## context switch

用户-内核的切换（通过系统调用或中断）到旧进程的内核线程，上下文（context）切换到当前CPU的调度器线程，上下文（context）切换到新进程的内核线程，以及trap返回到用户级进程。xv6调度器在每个CPU上有一个专门的线程(保存了寄存器和栈)，因为调度器在旧进程的内核栈上执行是不安全的：因为其他核心可能会唤醒该进程并运行它，而在两个不同的核心上使用相同的栈将是一场灾难。


从一个线程切换到另一个线程，需要保存旧线程的CPU寄存器，并恢复新线程之前保存的寄存器；栈指针和pc被保存和恢复，意味着CPU将切换栈和正在执行的代码。

函数swtch执行内核线程切换的保存和恢复。swtch并不直接知道线程，它只是保存和恢复寄存器组，称为上下文(context)。当一个进程要放弃CPU的时候，进程的内核线程会调用swtch保存自己的上下文并返回到调度器上下文。每个上下文都包含在一个结构体 context(kernel/proc.h:2)中，它本身包含在进程的结构体proc或CPU的结构体cpu中。Swtch有两个参数：struct context old和struct context new。它将当前的寄存器保存在old中，从new中加载寄存器，然后返回。

Swtch(kernel/swtch.S:3)只保存callee-saved寄存器，caller-saved寄存器由调用的C代码保存在堆栈上(如果需要)。Swtch知道struct context中每个寄存器字段的偏移量。它不保存pc。相反，swtch保存了ra寄存器[1]，它保存了swtch应该返回的地址。现在，swtch从新的上下文中恢复寄存器，新的上下文中保存着前一次swtch所保存的寄存器值。当swtch返回时，它返回到被恢复的ra寄存器所指向的指令，也就是新线程之前调用swtch的指令。此外，它还会返回新线程的堆栈。

在我们的例子中，sched调用swtch切换到cpu->scheduler，即CPU调度器的上下文。这个上下文已经被scheduler对swtch的调用所保存(kernel/proc.c:475)。当我们跟踪的swtch返回时，它不是返回到sched而是返回到scheduler，它的栈指针指向当前CPU的调度器栈。

## scheduling

我们刚刚看到xv6在调用swtch的过程中持有p->lock：swtch的调用者必须已经持有锁，并把锁的控制权移交给切换到的代码。这种约定对于锁来说是不寻常的；一般来说获得锁的线程也要负责释放锁，这样才容易保证正确性。对于上下文切换来说，有必要打破这个约定，因为p->lock保护了进程的状态和context字段上的不变量（invariant），而这些不变量在swtch执行时是不正确的。如果p->lock在swtch过程中不被持有，可能会出现问题的一个情况：在yield将其状态设置为RUNNABLE之后，但在swtch切换到新的栈之前，其他CPU可能会运行这个进程。结果就是两个CPU运行在同一个栈上，这显然是错误的。

一个内核线程在sched中放弃它的CPU，并且切换到scheduler的同一个位置，而scheduler（几乎）总是切换到之前调用sched的某个内核线程。因此，如果把xv6切换线程的行号打印出来，就会观察到下面的结果：(kernel/proc.c:475)，(kernel/proc.c:509)，(kernel/proc.c:475)，(kernel/proc.c:509)，等等。在两个线程之间发生这种样式化切换的程序有时被称为协程（coroutine）；在这个例子中，sched和scheduler是彼此的coroutines。

有一种情况是调度器对swtch的调用没有以sched结束。当一个新进程第一次被调度时，它从forkret开始（kernel/proc.c:527）。forkret的存在是为了释放p->lock；否则，新进程需要从usertrapret开始。


scheduler(kernel/proc.c:457)运行了一个简单的循环：找到一个可以运行进程，运行它，直到它让出CPU，一直重复。调度器在进程表上循环寻找一个可运行的进程，即p->state == RUNNABLE的进程。一旦找到这样的进程，它就会设置CPU当前进程变量c->proc指向该进程，将该进程标记为RUNNING，然后调用swtch开始运行它(kernel/proc.c:470- 475)。

## mycpu & myproc 

​ Xv6经常需要一个指向当前进程proc的指针。在单核处理器上，可以用一个全局变量指向当前的proc。这在多核机器上是行不通的，因为每个核都执行不同的进程。解决这个问题的方法是利用每个核都有自己的一组寄存器的事实；我们可以使用其中的一个寄存器来帮助查找每个核的信息。

Xv6为每个CPU维护了一个cpu结构体(kernel/proc.h:22)，它记录了当前在该CPU上运行的进程(如果有的话)，为CPU的调度线程保存的寄存器，以及管理中断禁用所需的嵌套自旋锁的计数。函数mycpu(kernel/proc.c:60)返回一个指向当前CPU结构体cpu的指针。RISC-V对CPU进行编号，给每个CPU一个hartid。Xv6确保每个CPU的hartid在内核中被存储在该CPU的tp寄存器中。这使得mycpu可以使用tp对cpu结构体的数组进行索引，从而找到正确的cpu。

确保一个CPU的tp始终保持CPU的hartid是有一点复杂的。mstart在CPU启动的早期设置tp寄存器，此时CPU处于机器模式(kernel/start.c:46)。Usertrapret将tp寄存器保存在trampoline页中，因为用户进程可能会修改tp寄存器。最后，当从用户空间进入内核时，uservec会恢复保存的tp（trapframe中的tp加载到tp寄存器）(kernel/trampoline.S:70)。编译器保证永远不使用tp寄存器。如果RISC-V允许xv6直接读取当前的hartid会更方便，但这只允许在机器模式下读取，而不允许在管理模式下读取。

cpuid和mycpu的返回值很容易错：如果定时器中断，导致线程让出CPU，然后转移到不同的CPU上，之前返回的值将不再正确。为了避免这个问题，xv6要求调用者禁用中断，只有在使用完返回的cpu结构后才启用中断。(即为了避免这个问题，调用cpuid和mycpu时，需要禁用中断)

myproc(kernel/proc.c:68)函数返回当前CPU上运行的进程的proc指针。myproc禁用中断，调用mycpu，从struct cpu中获取当前进程指针(c->proc)，然后启用中断。即使启用了中断，myproc的返回值也可以安全使用：如果定时器中断将调用进程移到了另一个的CPU上，它的proc结构指针将保持不变。

## sleep & wakeup 

调度和锁有助于让一个进程对另一个进程的不可见，但到目前为止，我们还没有帮助进程进行交互的抽象。人们发明了许多机制来解决这个问题。Xv6使用了一种叫做睡眠和唤醒的机制，它允许一个进程睡眠并等待事件，另一个进程在该事件发生后将其唤醒。睡眠和唤醒通常被称为序列协调（sequence coordination） 或条件同步（conditional synchronization） 机制。

为了说明这一点，让我们考虑一个叫做信号量（semaphore）[4]的同步机制，它协调生产者和消费者。信号量维护一个计数并提供两个操作。V操作（针对生产者）增加计数。P操作（针对消费者）等待，直到计数非零，然后将其递减并返回。如果只有一个生产者线程和一个消费者线程，而且它们在不同的CPU上执行，编译器也没有太过激进的优化，那么这个实现是正确的。

```c 
struct semaphore
{
  struct spinlock lock;
  int count;
};

void V(struct semaphore *s)
{
  acquire(&s->lock);
  s->count += 1;
  release(&s->lock);
}

void P(struct semaphore *s)
{
  while (s->count == 0)
    ;
  acquire(&s->lock);
  s->count -= 1;
  release(&s->lock);
}
```

# pipes 

一个使用sleep和wakeup来同步生产者和消费者的更复杂的例子是xv6的管道实现。我们在第1章看到了管道的接口：写入管道一端的字节被复制到内核缓冲区，然后可以从管道的另一端读取。未来的章节将研究管道如何支持文件描述符，但我们现在来看一下pipewrite和piperead的实现吧。

每个管道由一个结构体 pipe表示，它包含一个锁和一个数据缓冲区。nread和nwrite两个字段统计从缓冲区读取和写入的字节总数。缓冲区呈环形：buf[PIPESIZE-1]之后写入的下一个字节是buf[0]。计数不呈环形。这个约定使得实现可以区分满缓冲区(nwrite == nread+PIPESIZE)和空缓冲区(nwrite == nread)，但这意味着对缓冲区的索引必须使用buf[nread % PIPESIZE]，而不是使用buf[nread](nwrite也是如此)。

假设对piperead和pipewrite的调用同时发生在两个不同的CPU上。Pipewrite (kernel/pipe.c:77)首先获取管道的锁，它保护了计数、数据和相关的不变式。然后，Piperead (kernel/pipe.c:103)也试图获取这个锁，但是不会获取成功。它在acquire(kernel/spinlock.c:22)中循环，等待锁的到来。当piperead等待时，pipewrite会循环写，依次将每个字节添加到管道中(kernel/pipe.c:95)。在这个循环中，可能会发生缓冲区被填满的情况(kernel/pipe.c:85)。在这种情况下，pipewrite调用wakeup来提醒所有睡眠中的reader有数据在缓冲区中等待，然后在**&pi->nwrite上sleep**，等待reader从缓冲区中取出一些字节。Sleep函数内会释放pi->lock，然后pipwrite进程睡眠。

现在pi->lock可用了，piperead设法获取它并进入它的临界区：它发现pi->nread != pi->nwrite (kernel/pipe.c:110) (pipewrite进入睡眠状态是由于pi->nwrite == pi->nread+PIPESIZE (kernel/pipe.c:85))，所以它进入for循环，将数据从管道中复制出来**(kernel/pipe.c:117)，并按复制的字节数增加nread**。现在又可写了，所以 piperead 在返回之前调用 wakeup (kernel/pipe.c:124) 来唤醒在睡眠的writer。Wakeup找到一个在**&pi->nwrite上睡眠的进程，这个进程正在运行pipewrite**，但在缓冲区填满时停止了。它将该进程标记为RUNNABLE。

管道代码对reader和writer分别使用不同的睡眠channel（pi->nread和pi->nwrite）；这可能会使系统在有多个reader和writer等待同一个管道的情况下更有效率。管道代码在循环内sleep，检查sleep条件；如果有多个reader 和 writer，除了第一个被唤醒的进程外，其他进程都会看到条件仍然是假的，然后再次睡眠。

