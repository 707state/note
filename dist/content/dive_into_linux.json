{
  "path": "content/dive_into_linux.md",
  "modified_at_unix": 1772198335,
  "metadata": {
    "title": "从Linux 2.6搞明白Linux",
    "author": "jask",
    "tags": [
      "Linux",
      "OS"
    ],
    "date": "2026-01-23"
  },
  "content": "<h1>启动</h1>\n<h2>startup</h2>\n<p>Linux的启动流程在不同的架构上不太一样，这里以i386为例。</p>\n<p>计算机开机时，BIOS/UEFI把引导程序(比如GRUB)加载到内存，GRUB会加载Linux内核，在i386中，内核被加载时，第一个代码入口是arch/x86/kernel/head_32.S里面的startup_32。</p>\n<p><img src=\"images/linux26/startup_32.png\" alt=\"startup_32\" /></p>\n<p>在startup阶段，要完成从实模式到保护模式的过渡，还需要进行早期的硬件初始化，包括：重新加载段寄存器，复制引导参数，初始化页表，设置虚拟内存映射，启用分页并跳转到内核代码的起始位置，最终会跳转到i386_start_kernel函数。接下来会完成后续的内核初始化。</p>\n<h2>start_kernel</h2>\n<p>在i386_start_kernel函数中会根据具体的subarch选择对应的early_setup实现。</p>\n<p><img src=\"images/linux26/start_kernel.png\" alt=\"start_kernel\" /></p>\n<p>在这段函数的开头会预留足够的内存，确保初始化期间的一些关键内存区域不会被覆盖，内核会根据引导参数hardware_subarch字段执行特定硬件平台的初始化函数。</p>\n<p>在此之后, 所有必要的内存区域都被标记为了预留或者不可用，内核已经完成了内存预留(代码段、数据段、BSS段还有RAM都足够)，引导程序和BIOS相关的内存也被标记为不可用，避免了干扰。</p>\n<p>start_kernel函数负责执行大部分早期的初始化工作来让内核正常运行，里面执行了大量的xxx_init操作，比如tick_init, boot_cpu_init等等。</p>\n<p>start_kernel里面最重要的几个函数是:</p>\n<ol>\n<li>setup_arch:</li>\n</ol>\n<p>这一个函数负责把引导程序/BIOS/EFI提供的原始信息变为内核可用的结构，把系统的物理内存计算出来(max_low_pfn_mapped、max_pfn_mapped)，ACPI解析，NUMA节点拓扑发现等等都在这里完成，还有APIC/IOAPIC映射和IRQ数量探测。</p>\n<ol start=\"2\">\n<li>build_all_zonelists和page_alloc_init:</li>\n</ol>\n<p>在这两个函数之前内存分配只能使用bootmem，而在这两个函数前者会对内存的物理区域进行划分和分组，后者则为多核系统的内存分配通知系统做准备。</p>\n<pre><code class=\"language-c\">void __init page_alloc_init(void)\n{\n\thotcpu_notifier(page_alloc_cpu_notify, 0);\n}\nstatic int page_alloc_cpu_notify(struct notifier_block *self,\n\t\t\t\t unsigned long action, void *hcpu)\n{\n\tint cpu = (unsigned long)hcpu;\n\n\tif (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {\n\t\tdrain_pages(cpu);\n\n\t\t/*\n\t\t * Spill the event counters of the dead processor\n\t\t * into the current processors event counters.\n\t\t * This artificially elevates the count of the current\n\t\t * processor.\n\t\t */\n\t\tvm_events_fold_cpu(cpu);\n\n\t\t/*\n\t\t * Zero the differential counters of the dead processor\n\t\t * so that the vm statistics are consistent.\n\t\t *\n\t\t * This is only okay since the processor is dead and cannot\n\t\t * race with what we are doing.\n\t\t */\n\t\trefresh_cpu_vm_stats(cpu);\n\t}\n\treturn NOTIFY_OK;\n}\n</code></pre>\n<p>可以看到这里面注册的回调函数会在CPU状态改变时触发。</p>\n<ol start=\"3\">\n<li>trap_init和init_IRQ</li>\n</ol>\n<p>trap_init会为特殊的中断号设置对应的中断服务例程。</p>\n<p><img src=\"images/linux26/trap_init.png\" alt=\"trap_init\" /></p>\n<p>在i386下，这些例程都被定义在entry_32.S中。</p>\n<p>init_IRQ会完成分配和设置IRQ向量，并且完成APIC和IOAPIC相关的初始化。</p>\n<ol start=\"4\">\n<li>sched_init</li>\n</ol>\n<p>sched_init要把内核的调度器(包括调度队列, 调度策略, 任务组和运行队列)初始化。</p>\n<ul>\n<li>\n<p>初始化了公平调度（CFS）和实时调度（RT）的相关数据结构。</p>\n</li>\n<li>\n<p>配置了 CPU 负载、调度器亲和性、CPU 的调度队列（rq）。</p>\n</li>\n<li>\n<p>为系统中的每个 CPU 设置了默认的调度策略和任务负载。</p>\n</li>\n<li>\n<p>配置了内核空闲线程、任务组、调度类和性能监控。</p>\n</li>\n</ul>\n<ol start=\"5\">\n<li>rcu_init</li>\n</ol>\n<p>Read-Copy-Update是Linux内核里面处理同步的重要机制。</p>\n<p><img src=\"images/linux26/rcu_init.png\" alt=\"rcu_init\" /></p>\n<p>open_softirq为内核注册了软中断，cpu_notifier会在CPU状态变更时通知RCPU进行必要的修改。</p>\n<ol start=\"6\">\n<li>rest_init</li>\n</ol>\n<p>rest_init里面最重要的部分是kernel_thread(kernel_init)。这个函数启动了内核线程并执行系统初始化任务。在kernel_init最后会执行init_post，这里会执行run_init_process，通过kernel_execve把init进程跑起来。</p>\n<p><img src=\"images/linux26/run_init_process.png\" alt=\"run_init_process\" /></p>\n<p>在2.6内核中，除了init进程是内核启动的，还有kthreadadd这个PID为2的任务也是内核启动的。主要管理待创建的内核线程。</p>\n<h1>进程/线程</h1>\n<p>Linux创建一个新进程的方式是fork+exec，尽管POSIX对于各个系统的原语进行了封装，Unix-Like的系统基本上都不提供直接的spawn操作。</p>\n<p>Linux采用Copy-On-Write的方式实现fork，fork本身的开销就只有复制父进程的page tables和创建一个unique child pid，而不包括立刻复制物理内存页。在fork后子进程向内存修改之前，父子进程的page table都指向同一批物理页。只有当子进程修改内存时才会发生物理页的复制。</p>\n<p>Unix是鼓励迅速的进程创建的，也就是fork + exec的共同使用。exec会丢弃当前进程的整个用户态空间，加载ELF并建立新的page tables。根本没有必要复制父进程物理页。 所以Linux的COW减少了fork+exec时不必要的内存开销。</p>\n<p>Linux上fork是copy的一个特例, vfork则是copy的另一种特例。 当然三者都是调用内核的do_fork。</p>\n<h2>do_fork</h2>\n<p><img src=\"images/linux26/do_fork.png\" alt=\"do_fork\" /></p>\n<p>这里面最重要的部分是copy_process。</p>\n<pre><code class=\"language-c\">static struct task_struct *copy_process(unsigned long clone_flags,\n                    unsigned long stack_start,\n\t\t\t\t\tstruct pt_regs *regs,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace){\n                    ...\n    ret=dup_task_struct(current);// 创建新的内核栈、thread_info、task_struct，此时父子进程pid还没有区别\n    ...\n    // 检查新进程是否会超出资源限制\n    if (atomic_read(&amp;p-&gt;real_cred-&gt;user-&gt;processes) &gt;=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) &amp;&amp; !capable(CAP_SYS_RESOURCE) &amp;&amp;\n\t\t    p-&gt;real_cred-&gt;user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n    ...\n    // 还在检查\n    retval = -EAGAIN;\n\tif (nr_threads &gt;= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)-&gt;exec_domain-&gt;module))\n\t\tgoto bad_fork_cleanup_count;\n    ...\n    // 把子进程从父进程的按位拷贝快照里区分出来，也就是给子任务结构体赋值\n    p-&gt;did_exec = 0;\n    delayacct_tsk_init(p);  /* Must remain after dup_task_struct() */\n    copy_flags(clone_flags, p);\n    INIT_LIST_HEAD(&amp;p-&gt;children);\n    ...\n    init_sigpending(&amp;p-&gt;pending);\n    ...\n    p-&gt;utime = cputime_zero;\n    p-&gt;stime = cputime_zero;\n    ...\n    do_posix_clock_monotonic_gettime(&amp;p-&gt;start_time);\n    p-&gt;real_start_time = p-&gt;start_time;\n    ...\n    task_io_accounting_init(&amp;p-&gt;ioac);\n    acct_clear_integrals(p);\n    posix_cpu_timers_init(p);\n    ...\n    sched_fork(p, clone_flags);\n    ...\n    // 分配pid\n    if (pid != &amp;init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p-&gt;nsproxy-&gt;pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\n\t\tif (clone_flags &amp; CLONE_NEWPID) {\n\t\t\tretval = pid_ns_prepare_proc(p-&gt;nsproxy-&gt;pid_ns);\n\t\t\tif (retval &lt; 0)\n\t\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t}\n\n\tp-&gt;pid = pid_nr(pid);\n\tp-&gt;tgid = p-&gt;pid;\n    ...\n    //根据clone flags决定资源是共享还是复制\n    \tif ((retval = copy_semundo(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_audit;\n\tif ((retval = copy_files(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_semundo;\n\tif ((retval = copy_fs(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_files;\n\tif ((retval = copy_sighand(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_fs;\n\tif ((retval = copy_signal(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_sighand;\n\tif ((retval = copy_mm(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_signal;\n\tif ((retval = copy_namespaces(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_mm;\n\tif ((retval = copy_io(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_namespaces;\n    ...\n    // 收尾，把新的task添加到系统、挂在各种链表/哈希表里面、处理线程/进程组/session之间的关系.\n    return p;\n}\n</code></pre>\n<p>copy_process执行成功之后，do_fork里父进程需要立即trace并获取child pid，把子进程写回给父进程的用户空间，还要去通知audit/tracing子系统clone已经发生。根据clone_flags还要决定子任务是STOP还是RUNNABLE(wake_up_new_task)。vfork的逻辑稍微特殊一点，父进程要阻塞等待完成。</p>\n<p><img src=\"images/linux26/after_copy_process.png\" alt=\"after_copy_process\" /></p>\n<p>有个说法是：<em>fork有两个返回值，一个在父进程返回，一个在子进程返回</em>。</p>\n<p>这个说法有失偏颇。</p>\n<p>先看父进程：</p>\n<pre><code class=\"language-c\">\tp = copy_process(clone_flags, stack_start, regs, stack_size,\n\t\t\t child_tidptr, NULL, trace);\n\t/*\n\t * Do this prior waking up the new thread - the thread pointer\n\t * might get invalid after that point, if the thread exits quickly.\n\t */\n\tif (!IS_ERR(p)) {\n\t\tstruct completion vfork;\n\n\t\ttrace_sched_process_fork(current, p);\n\n\t\tnr = task_pid_vnr(p);\n        ...\n\t} else {\n\t\tnr = PTR_ERR(p);\n\t}\n\treturn nr;\n</code></pre>\n<p>task_pid_vnr会从父进程的PID namespace里面获取到子进程的PID, 在成功是返回相应的pid。这是第一个返回值。</p>\n<p>在哪里给子进程返回呢？</p>\n<p><img src=\"images/linux26/copy_thread.png\" alt=\"copy_thread\" /></p>\n<p>子进程的返回发生在copy_thread里面，在childregs=task_pt_regs(p)这里获取到子进程内核栈上的pt_regs，接下来*childregs=*regs则是复制父进程的寄存器，而在childregs-&gt;ax=0这里就把子进程的返回值设置为0。在父进程执行完wake_up_new_task以后，子进程变成runnable的，调度器某个时刻选择子任务，然后context switch到子任务上，CPU开始执行p-&gt;thread.ip上的代码，最后通过通过ret_from_fork返回用户态。</p>\n<p><img src=\"images/linux26/ret_from_fork.png\" alt=\"ret_from_fork\" /></p>\n<p>这是x86上新创建的子进程/线程第一次被调度运行时的起点，由调度器切换到新任务时的thread.ip指向。</p>\n<h2>vfork</h2>\n<p>vfork相比fork，最大的区别就是vfork的子进程不会拷贝page tables entries，而且会一直阻塞父进程知道执行完成。</p>\n<h2>Threads</h2>\n<p>Linux内核并不会特殊对待线程，都是一个task_struct，基本上可以看作是共享一些资源的进程。</p>\n<p>Linux中有Kernel Thread，提供了内核中执行后台任务的能力，这些内核线程只会在内核态运行。</p>\n<p><img src=\"images/linux26/kthread_create.png\" alt=\"kthread_create\" /></p>\n<p>这里会看到一个kthreadadd_task，这是由rest_init（参考前面启动阶段）启动的第二个进程（pid为2）关联的task_struct，为了添加一个kernel thread去执行任务，在这里就需要唤醒kthreadadd这个进程。</p>\n<h2>exit</h2>\n<p>进程的结束通常是自行退出，即主动调用exit（即使不在main结尾放一个exit，编译器也会在main返回后插入一个）。</p>\n<p>exit的具体逻辑在do_exit中，大致流程：</p>\n<pre><code class=\"language-c\">void do_exit(long code){\n    struct task_struct *tsk = current;\n\tint group_dead;\n    ...\n    // 设置PF_EXITING\n    exit_irq_thread();\n\texit_signals(tsk);  /* sets PF_EXITING */\n    ...\n    acct_update_integrals(tsk); // 统计信息\n    ...\n    // 释放当前进程的mm_struct\n    tsk-&gt;exit_code = code;\n\ttaskstats_exit(tsk, group_dead);\n\texit_mm(tsk);\n    ...\n    exit_sem(tsk);\n    // 减少计数，计数归0时销毁对应的资源\n\texit_files(tsk);\n\texit_fs(tsk);\n    ...\n    exit_notify(tsk, group_dead); // 通知parent process\n    ...\n    exit_rcu();\n\t/* causes final put_task_struct in finish_task_switch(). */\n\ttsk-&gt;state = TASK_DEAD;\n\tschedule();\n}\n</code></pre>\n<p>执行了do_exit之后，所有资源都会被释放，现在只有thread_info还没被释放（如果所有的资源只被这个任务使用），在do_exit之后内核还是会保留pid，但是进程会变成zombie进程并且不能运行了。只有在release_task被调用时，才会清理PID。</p>\n<p><img src=\"images/linux26/release_task.png\" alt=\"release_task\" /></p>\n<p>具体的逻辑是：__exit_signal调用__unhash_process来执行detach_pid。</p>\n"
}