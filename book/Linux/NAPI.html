<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>NAPI - 学习笔记</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././mdbook-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">学习笔记</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <ul>
<li><a href="#linux%E7%BD%91%E7%BB%9C">Linux网络</a>
<ul>
<li><a href="#%E9%A9%B1%E5%8A%A8%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96">驱动加载与初始化</a></li>
<li><a href="#%E7%BD%91%E5%8D%A1%E6%95%B0%E6%8D%AE%E6%8E%A5%E6%94%B6">网卡数据接收</a>
<ul>
<li><a href="#%E5%A4%84%E7%90%86%E7%BD%91%E5%8D%A1%E4%B8%AD%E6%96%AD">处理网卡中断</a></li>
<li><a href="#%E5%90%AF%E5%8A%A8napi%E4%BB%BB%E5%8A%A1%E5%8F%91%E9%80%81%E8%BD%AF%E4%B8%AD%E6%96%AD">启动napi任务，发送软中断</a></li>
<li><a href="#%E5%A4%84%E7%90%86%E7%BD%91%E7%BB%9C%E8%BD%AF%E4%B8%AD%E6%96%AD">处理网络软中断</a></li>
<li><a href="#%E5%B0%86skb_buff-%E5%8F%91%E9%80%81%E7%BB%99%E5%8D%8F%E8%AE%AE%E6%A0%88">将skb_buff发送给协议栈</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="linux网络"><a class="header" href="#linux网络">Linux网络</a></h1>
<p>NAPI(New
API)是Linux内核针对网络数据传输做出的一个优化措施，其目的是在高负载的大数据传输时，网络驱动收到硬件中断后，通过poll(轮询)方式将传输过来的数据包统一处理，
在poll时通过禁止网络设备中断以减少硬件中断数量(Interrupt
Mitigation)，从而实现更高的数据传输速率。</p>
<p>基于NAPI接口， 一般的网络传输(接收)有如下几个步骤：</p>
<blockquote>
<p>网络设备驱动加载与初始化（配置IP等）</p>
</blockquote>
<blockquote>
<p>数据包从网络侧发送到网卡(Network Interface Controller, NIC)</p>
</blockquote>
<blockquote>
<p>通过DMA(Direct Memory Access)，将数据从网卡拷贝到内存的环形缓冲区(ring
buffer)</p>
</blockquote>
<blockquote>
<p>数据从网卡拷贝到内存后, NIC产生硬件中断告知内核有新的数据包达到</p>
</blockquote>
<blockquote>
<p>内核收到中断后, 调用相应中断处理函数,
此时就会调用NAPI接口__napi_schedule开启poll线程（实际是触发一个软中断NET_RX_SOFTIRQ）(常规数据传输,
一般在处理NIC的中断时调用netif_rx_action处理网卡队列的数据）</p>
</blockquote>
<blockquote>
<p>ksoftirqd（每个CPU上都会启动一个软中断处理线程）收到软中断后被唤醒,
然后执行函数net_rx_action,
这个函数负责调用NAPI的poll接口来获取内存环形缓冲区的数据包</p>
</blockquote>
<blockquote>
<p>解除网卡ring buffer中的DMA内存映射(unmapped), 数据由CPU负责处理,
netif_receive_skb传递回内核协议栈</p>
</blockquote>
<blockquote>
<p>如果内核支持数据包定向分发(packet
steering)或者NIC本身支持多个接收队列的话,
从网卡过来的数据会在不同的CPU之间进行均衡, 这样可以获得更高的网络速率</p>
</blockquote>
<blockquote>
<p>网络协议栈处理数据包，并将其发送到对应应用的socket接收缓冲区</p>
</blockquote>
<h2 id="驱动加载与初始化"><a class="header" href="#驱动加载与初始化">驱动加载与初始化</a></h2>
<p>驱动的初始化首先要做的是注册一个pci设备驱动到内核，这样系统启动时会根据PCI的ID列表匹配到该网卡,
并执行设备枚举:</p>
<p>e1000_main.c</p>
<pre><code class="language-c">static int __init e1000_init_module(void)
{
    int ret;
    pr_info("%s - version %s\n", e1000_driver_string, e1000_driver_version);

    pr_info("%s\n", e1000_copyright);

    ret = pci_register_driver(&amp;e1000_driver);
    ...
    return ret;
}

module_init(e1000_init_module);


static struct pci_driver e1000_driver = {
    .name     = e1000_driver_name,
    .id_table = e1000_pci_tbl,
    .probe    = e1000_probe,
    .remove   = e1000_remove,
#ifdef CONFIG_PM
    /* Power Management Hooks */
    .suspend  = e1000_suspend,
    .resume   = e1000_resume,
#endif
    .shutdown = e1000_shutdown,
    .err_handler = &amp;e1000_err_handler
};
</code></pre>
<p>匹配到网卡后， pci总线会调用驱动的probe函数， 大致会做如下几个事情：</p>
<blockquote>
<p>调用alloc_etherdev 分配一个网络设备对象，并注册到系统中</p>
</blockquote>
<blockquote>
<p>通过netif_napi_add添加NAPI的poll接口</p>
</blockquote>
<blockquote>
<p>设置网卡寄存器IO映射内存区域, 用于配置网卡</p>
</blockquote>
<pre><code class="language-c">static int e1000_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
    ....
    err = pci_request_selected_regions(pdev, bars, e1000_driver_name);
    if (err)
        goto err_pci_reg;

    pci_set_master(pdev);
    err = pci_save_state(pdev);
    if (err)
        goto err_alloc_etherdev;

    err = -ENOMEM;
    // 分配网络对象
    netdev = alloc_etherdev(sizeof(struct e1000_adapter));
    if (!netdev)
        goto err_alloc_etherdev;

    SET_NETDEV_DEV(netdev, &amp;pdev-&gt;dev);

    pci_set_drvdata(pdev, netdev);
    adapter = netdev_priv(netdev);
    adapter-&gt;netdev = netdev;
    adapter-&gt;pdev = pdev;
    adapter-&gt;msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);
    adapter-&gt;bars = bars;
    adapter-&gt;need_ioport = need_ioport;

    hw = &amp;adapter-&gt;hw;
    hw-&gt;back = adapter;

    err = -EIO;
    // 映射寄存器IO区域
    hw-&gt;hw_addr = pci_ioremap_bar(pdev, BAR_0);
    if (!hw-&gt;hw_addr)
        goto err_ioremap;

    if (adapter-&gt;need_ioport) {
        for (i = BAR_1; i &lt;= BAR_5; i++) {
            if (pci_resource_len(pdev, i) == 0)
                continue;
            if (pci_resource_flags(pdev, i) &amp; IORESOURCE_IO) {
                hw-&gt;io_base = pci_resource_start(pdev, i);
                break;
            }
        }
    }

    /* make ready for any if (hw-&gt;...) below */
    err = e1000_init_hw_struct(adapter, hw);
    if (err)
        goto err_sw_init;   
    ...
    // 设置网络设备对象的操作接口
    netdev-&gt;netdev_ops = &amp;e1000_netdev_ops;
    e1000_set_ethtool_ops(netdev);
    netdev-&gt;watchdog_timeo = 5 * HZ;
    // 添加napi的poll接口
    netif_napi_add(netdev, &amp;adapter-&gt;napi, e1000_clean, 64);

    strncpy(netdev-&gt;name, pci_name(pdev), sizeof(netdev-&gt;name) - 1);

    adapter-&gt;bd_number = cards_found;

    /* setup the private structure */

    err = e1000_sw_init(adapter);
    if (err)
        goto err_sw_init;
    ...

    if (!is_valid_ether_addr(netdev-&gt;dev_addr))
        e_err(probe, "Invalid MAC Address\n");


    INIT_DELAYED_WORK(&amp;adapter-&gt;watchdog_task, e1000_watchdog);
    INIT_DELAYED_WORK(&amp;adapter-&gt;fifo_stall_task,
              e1000_82547_tx_fifo_stall_task);
    INIT_DELAYED_WORK(&amp;adapter-&gt;phy_info_task, e1000_update_phy_info_task);
    INIT_WORK(&amp;adapter-&gt;reset_task, e1000_reset_task);
    ...

    /* initialize the wol settings based on the eeprom settings */
    adapter-&gt;wol = adapter-&gt;eeprom_wol;
    device_set_wakeup_enable(&amp;adapter-&gt;pdev-&gt;dev, adapter-&gt;wol);

    /* Auto detect PHY address */
    if (hw-&gt;mac_type == e1000_ce4100) {
        for (i = 0; i &lt; 32; i++) {
            hw-&gt;phy_addr = i;
            e1000_read_phy_reg(hw, PHY_ID2, &amp;tmp);
            if (tmp == 0 || tmp == 0xFF) {
                if (i == 31)
                    goto err_eeprom;
                continue;
            } else
                break;
        }
    }

    /* reset the hardware with the new settings */
    e1000_reset(adapter);
    
    // 设置网卡名字，注册网络设备对象
    strcpy(netdev-&gt;name, "eth%d");
    err = register_netdev(netdev);
    if (err)
        goto err_register;
    ...
    /* carrier off reporting is important to ethtool even BEFORE open */
    netif_carrier_off(netdev);

    e_info(probe, "Intel(R) PRO/1000 Network Connection\n");

    cards_found++;
    return 0;
    ...
}
</code></pre>
<p>到这一步网卡还不是可用状态，需要通过用户配置网卡(开启网卡以及配置IP),
如ifconfig eth0 up/ifconfig eth0 <code>&lt;ip&gt;</code>{=html}.
设置网卡为UP时，会调用驱动的ndo_open函数：</p>
<blockquote>
<p>请求硬件中断，并使能该中断</p>
</blockquote>
<blockquote>
<p>napi_enable开启napi</p>
</blockquote>
<blockquote>
<p>启动网络的发送队列，允许发送数据</p>
</blockquote>
<pre><code class="language-c">static int e1000_open(struct net_device *netdev)
{
    struct e1000_adapter *adapter = netdev_priv(netdev);
    struct e1000_hw *hw = &amp;adapter-&gt;hw;
    int err;

    netif_carrier_off(netdev);

    /* allocate transmit descriptors */
    err = e1000_setup_all_tx_resources(adapter);

    /* allocate receive descriptors */
    err = e1000_setup_all_rx_resources(adapter);

    e1000_power_up_phy(adapter);
    
    ...
    err = e1000_request_irq(adapter);

    /* From here on the code is the same as e1000_up() */
    clear_bit(__E1000_DOWN, &amp;adapter-&gt;flags);

    napi_enable(&amp;adapter-&gt;napi);

    e1000_irq_enable(adapter);

    netif_start_queue(netdev);

    /* fire a link status change interrupt to start the watchdog */
    ew32(ICS, E1000_ICS_LSC);

    return E1000_SUCCESS;
    
    ...

    return err;
}
</code></pre>
<p>到此时网卡正常工作， 可以开始接发数据, 我们可以通过ifconfig
ethX来查看当前网卡的状态. 接下来, 再来看看数据接收的具体流程。</p>
<h2 id="网卡数据接收"><a class="header" href="#网卡数据接收">网卡数据接收</a></h2>
<p>网卡数据的接收大概有三个步骤：</p>
<blockquote>
<p>网卡发送中断给驱动</p>
</blockquote>
<blockquote>
<p>驱动处理函数处理中断，并启动一个napi处理任务</p>
</blockquote>
<blockquote>
<p>发送接收数据的软中断NET_RX_SOFTIRQ</p>
</blockquote>
<blockquote>
<p>内核线程处理网络软中断，将数据包发送给上层协议栈</p>
</blockquote>
<h3 id="处理网卡中断"><a class="header" href="#处理网卡中断">处理网卡中断</a></h3>
<p>在网卡驱动初始化的过程，我们看到驱动会向内核请求中断,
并注册一个中断处理函数：</p>
<pre><code class="language-c">static int e1000_request_irq(struct e1000_adapter *adapter)
{
    struct net_device *netdev = adapter-&gt;netdev;
    irq_handler_t handler = e1000_intr;
    int irq_flags = IRQF_SHARED;

    err = request_irq(adapter-&gt;pdev-&gt;irq, handler, irq_flags, netdev-&gt;name,
                  netdev);
        ...

    return err;
}
</code></pre>
<p>当网卡产生数据中断后，调用中断处理函数：</p>
<p>对于napi来说，首先要禁止当前网卡的中断，如果当前没有在运行的napi任务，则调度一个新的napi任务__napi_schedule：</p>
<pre><code class="language-c">static irqreturn_t e1000_intr(int irq, void *data)
{

    ...

    /* disable interrupts, without the synchronize_irq bit */
    ew32(IMC, ~0);
    E1000_WRITE_FLUSH();

    if (likely(napi_schedule_prep(&amp;adapter-&gt;napi))) {
        adapter-&gt;total_tx_bytes = 0;
        adapter-&gt;total_tx_packets = 0;
        adapter-&gt;total_rx_bytes = 0;
        adapter-&gt;total_rx_packets = 0;
        __napi_schedule(&amp;adapter-&gt;napi);
    } else {
        /* this really should not happen! if it does it is basically a
         * bug, but not a hard error, so enable ints and continue
         */
        if (!test_bit(__E1000_DOWN, &amp;adapter-&gt;flags))
            e1000_irq_enable(adapter);
    }

    return IRQ_HANDLED;
}
</code></pre>
<h3 id="启动napi任务发送软中断"><a class="header" href="#启动napi任务发送软中断">启动napi任务，发送软中断</a></h3>
<p>__napi_schedule在/kernel/net/core/dev.c中，其实际做了两件事：</p>
<blockquote>
<p>将napi_struct添加到中断处理CPU的softnet_data对应的poll列表中</p>
</blockquote>
<blockquote>
<p>发出一个NET_RX_SOFTIRQ的软中断，让内核线程ksoftirqd来处理对应的该softirq软中断</p>
</blockquote>
<pre><code class="language-c">
void __napi_schedule(struct napi_struct *n)
{
    unsigned long flags;
    local_irq_save(flags);
    ____napi_schedule(this_cpu_ptr(&amp;softnet_data), n);
    local_irq_restore(flags);
}
static inline void ____napi_schedule(struct softnet_data *sd,
                     struct napi_struct *napi)
{
    list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);
    __raise_softirq_irqoff(NET_RX_SOFTIRQ);
}
</code></pre>
<p>__raise_softirq_irqoff函数在/kernel/softirq.c中定义，其作用就是将当前CPU对应的softirq状态标记为待运行状态：</p>
<pre><code class="language-c">void __raise_softirq_irqoff(unsigned int nr)
{
    trace_softirq_raise(nr);
    or_softirq_pending(1UL &lt;&lt; nr);
}
</code></pre>
<h3 id="处理网络软中断"><a class="header" href="#处理网络软中断">处理网络软中断</a></h3>
<p>内核在初始化的时候，每个CPU上都会启动一个专门的ksoftirqd%d（%d对应CPU的ID）内核线程用于处理CPU上的软中断（代码同样在softirq.c)中：</p>
<pre><code class="language-c">static struct smp_hotplug_thread softirq_threads = {
    .store          = &amp;ksoftirqd,
    .thread_should_run  = ksoftirqd_should_run,
    .thread_fn      = run_ksoftirqd,
    .thread_comm        = "ksoftirqd/%u",
};

static __init int spawn_ksoftirqd(void)
{
    register_cpu_notifier(&amp;cpu_nfb);

    BUG_ON(smpboot_register_percpu_thread(&amp;softirq_threads));

    return 0;
}
early_initcall(spawn_ksoftirqd);
</code></pre>
<p>每个内核线程ksoftirqd实际一直执行的是run_ksoftirqd函数：</p>
<pre><code class="language-c">static void run_ksoftirqd(unsigned int cpu)
{
    local_irq_disable();
    if (local_softirq_pending()) {
        /*
         * We can safely run softirq on inline stack, as we are not deep
         * in the task stack here.
         */
        __do_softirq();
        local_irq_enable();
        cond_resched_rcu_qs();
        return;
    }
    local_irq_enable();
}
</code></pre>
<p>函数__do_softirq检查当前CPU所有待处理的软中断，并调用对应的处理函数softirq_action：</p>
<pre><code class="language-c">asmlinkage __visible void __softirq_entry __do_softirq(void)
{
    ...
    while ((softirq_bit = ffs(pending))) {
        unsigned int vec_nr;
        int prev_count;

        h += softirq_bit - 1;

        vec_nr = h - softirq_vec;
        ...
        h-&gt;action(h);
        ...
        h++;
        pending &gt;&gt;= softirq_bit;
    }
    ...
}
</code></pre>
<p>而softirq_action实际是在网络模块初始化的时候注册的(查看/kernel/net/dev.c)中的函数net_dev_init，通过调用open_softirq告知内核启动网络数据传输的两个软中断：</p>
<pre><code class="language-c">static int __init net_dev_init(void)
{
    ...
    if (register_pernet_subsys(&amp;netdev_net_ops))
        goto out;

    /*
     *  Initialise the packet receive queues.
     */

    for_each_possible_cpu(i) {
        struct softnet_data *sd = &amp;per_cpu(softnet_data, i);

        skb_queue_head_init(&amp;sd-&gt;input_pkt_queue);
        skb_queue_head_init(&amp;sd-&gt;process_queue);
        INIT_LIST_HEAD(&amp;sd-&gt;poll_list);
        sd-&gt;output_queue_tailp = &amp;sd-&gt;output_queue;
#ifdef CONFIG_RPS
        sd-&gt;csd.func = rps_trigger_softirq;
        sd-&gt;csd.info = sd;
        sd-&gt;cpu = i;
#endif

        sd-&gt;backlog.poll = process_backlog;
        sd-&gt;backlog.weight = weight_p;
    }

    ...

    open_softirq(NET_TX_SOFTIRQ, net_tx_action);
    open_softirq(NET_RX_SOFTIRQ, net_rx_action);

    hotcpu_notifier(dev_cpu_callback, 0);
    dst_subsys_init();
    rc = 0;
}
</code></pre>
<p>也就说h-&gt;action 调用的实际是net_rx_action函数：
检查当前CPU的softnet_data的poll_list, 取出第一个设备的napi列表,
调用napi_poll获取对应网卡上的数据包，每个设备执行poll会受到两个参数的约束(确保不会占用过多的CPU资源):</p>
<blockquote>
<p>netdev_budget_usecs: 每个设备能够处理的最大时间长度(默认是2000us)</p>
</blockquote>
<blockquote>
<p>netdev_budget: 单个设备一次能处理的最大包的配额(默认是300)</p>
</blockquote>
<p>一旦超过给定的时间限制或者处理的包达到配额上限,
则直接退出;如果当前poll_list不为空, 则再次触发一个软中断.</p>
<pre><code class="language-c">static void net_rx_action(struct softirq_action *h)
{
    struct softnet_data *sd = this_cpu_ptr(&amp;softnet_data);
    unsigned long time_limit = jiffies +
        usecs_to_jiffies(netdev_budget_usecs);
    int budget = netdev_budget;
    LIST_HEAD(list);
    LIST_HEAD(repoll);

    local_irq_disable();
    list_splice_init(&amp;sd-&gt;poll_list, &amp;list);
    local_irq_enable();
    for (;;) {
        struct napi_struct *n;

        if (list_empty(&amp;list)) {
            if (!sd_has_rps_ipi_waiting(sd) &amp;&amp; list_empty(&amp;repoll))
                return;
            break;
        }

        n = list_first_entry(&amp;list, struct napi_struct, poll_list);
        budget -= napi_poll(n, &amp;repoll);

        /* If softirq window is exhausted then punt.
         * Allow this to run for 2 jiffies since which will allow
         * an average latency of 1.5/HZ.
         */
        if (unlikely(budget &lt;= 0 ||
                 time_after_eq(jiffies, time_limit))) {
            sd-&gt;time_squeeze++;
            break;
        }
    }

    local_irq_disable();

    list_splice_tail_init(&amp;sd-&gt;poll_list, &amp;list);
    list_splice_tail(&amp;repoll, &amp;list);
    list_splice(&amp;list, &amp;sd-&gt;poll_list);
    if (!list_empty(&amp;sd-&gt;poll_list))
        __raise_softirq_irqoff(NET_RX_SOFTIRQ);

    net_rps_action_and_irq_enable(sd);
}
</code></pre>
<p>而napi_poll则调用最初网卡驱动注册的poll函数e1000_clean：</p>
<pre><code class="language-c">static int napi_poll(struct napi_struct *n, struct list_head *repoll)
{
    void *have;
    int work, weight;

    list_del_init(&amp;n-&gt;poll_list);

    have = netpoll_poll_lock(n);

    weight = n-&gt;weight;

    /* This NAPI_STATE_SCHED test is for avoiding a race
     * with netpoll's poll_napi().  Only the entity which
     * obtains the lock and sees NAPI_STATE_SCHED set will
     * actually make the -&gt;poll() call.  Therefore we avoid
     * accidentally calling -&gt;poll() when NAPI is not scheduled.
     */
    work = 0;
    if (test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)) {
        struct softnet_data *sd = this_cpu_ptr(&amp;softnet_data);

        sd-&gt;current_napi = n;
        work = n-&gt;poll(n, weight);
        trace_napi_poll(n);
    }
    
    ...

    return work;
}
</code></pre>
<p>函数e1000_clean调用clean_rx从网卡的DMA区域取出数据, 如果处理完成,
则重新开启设备中断:</p>
<pre><code class="language-c">static int e1000_clean(struct napi_struct *napi, int budget)
{
    struct e1000_adapter *adapter = container_of(napi, struct e1000_adapter,
                             napi);
    int tx_clean_complete = 0, work_done = 0;

    tx_clean_complete = e1000_clean_tx_irq(adapter, &amp;adapter-&gt;tx_ring[0]);

    adapter-&gt;clean_rx(adapter, &amp;adapter-&gt;rx_ring[0], &amp;work_done, budget);

    if (!tx_clean_complete)
        work_done = budget;

    /* If budget not fully consumed, exit the polling mode */
    if (work_done &lt; budget) {
        if (likely(adapter-&gt;itr_setting &amp; 3))
            e1000_set_itr(adapter);
        napi_complete_done(napi, work_done);
        if (!test_bit(__E1000_DOWN, &amp;adapter-&gt;flags))
            e1000_irq_enable(adapter);
    }

    return work_done;
}
</code></pre>
<h3 id="将skb_buff-发送给协议栈"><a class="header" href="#将skb_buff-发送给协议栈">将skb_buff 发送给协议栈</a></h3>
<p>e1000_clean_rx_irq不断的从网卡基于DMA地址从对应的内存环形缓冲区中获取网络数据包，并将数据包以sk_buff的形式传给协议栈进行处理.
简单来说, DMA地址就是外设(网卡)映射到内存的一个虚拟地址,
用于直接与内存(RAM)之间传送数据. 一般在驱动初始化的时候,
网卡通过dma_map_single为缓冲区的每个描述结构体中的数据映射到某个DMA地址,
网卡会把数据拷贝到该DMA地址;CPU收到中断后则数据交由CPU处理,
再通过dma_unmap_single清除掉这个DMA映射.</p>
<pre><code class="language-c">static bool e1000_clean_rx_irq(struct e1000_adapter *adapter,
                   struct e1000_rx_ring *rx_ring,
                   int *work_done, int work_to_do)
{
    struct net_device *netdev = adapter-&gt;netdev;
    struct pci_dev *pdev = adapter-&gt;pdev;
    struct e1000_rx_desc *rx_desc, *next_rxd;
    struct e1000_rx_buffer *buffer_info, *next_buffer;
    u32 length;
    unsigned int i;
    int cleaned_count = 0;
    bool cleaned = false;
    unsigned int total_rx_bytes=0, total_rx_packets=0;

    i = rx_ring-&gt;next_to_clean;
    rx_desc = E1000_RX_DESC(*rx_ring, i);
    buffer_info = &amp;rx_ring-&gt;buffer_info[i];

    while (rx_desc-&gt;status &amp; E1000_RXD_STAT_DD) {
        struct sk_buff *skb;
        u8 *data;
        u8 status;

        if (*work_done &gt;= work_to_do)
            break;
        (*work_done)++;
        dma_rmb(); /* read descriptor and rx_buffer_info after status DD */

        status = rx_desc-&gt;status;
        length = le16_to_cpu(rx_desc-&gt;length);

        data = buffer_info-&gt;rxbuf.data;
        prefetch(data);
        skb = e1000_copybreak(adapter, buffer_info, length, data);
        if (!skb) {
            unsigned int frag_len = e1000_frag_len(adapter);

            skb = build_skb(data - E1000_HEADROOM, frag_len);
            if (!skb) {
                adapter-&gt;alloc_rx_buff_failed++;
                break;
            }

            skb_reserve(skb, E1000_HEADROOM);
            // 清除DMA地址的映射, 这时CPU开始接管相应的数据
            dma_unmap_single(&amp;pdev-&gt;dev, buffer_info-&gt;dma,
                     adapter-&gt;rx_buffer_len,
                     DMA_FROM_DEVICE);
            buffer_info-&gt;dma = 0;
            buffer_info-&gt;rxbuf.data = NULL;
        }

        if (++i == rx_ring-&gt;count) i = 0;
        next_rxd = E1000_RX_DESC(*rx_ring, i);
        prefetch(next_rxd);

        next_buffer = &amp;rx_ring-&gt;buffer_info[i];

        cleaned = true;
        cleaned_count++;

        /* !EOP means multiple descriptors were used to store a single
         * packet, if thats the case we need to toss it.  In fact, we
         * to toss every packet with the EOP bit clear and the next
         * frame that _does_ have the EOP bit set, as it is by
         * definition only a frame fragment
         */
        if (unlikely(!(status &amp; E1000_RXD_STAT_EOP)))
            adapter-&gt;discarding = true;

        if (adapter-&gt;discarding) {
            /* All receives must fit into a single buffer */
            netdev_dbg(netdev, "Receive packet consumed multiple buffers\n");
            dev_kfree_skb(skb);
            if (status &amp; E1000_RXD_STAT_EOP)
                adapter-&gt;discarding = false;
            goto next_desc;
        }
        ....

process_skb:
        total_rx_bytes += (length - 4); /* don't count FCS */
        total_rx_packets++;

        if (likely(!(netdev-&gt;features &amp; NETIF_F_RXFCS)))
            /* adjust length to remove Ethernet CRC, this must be
             * done after the TBI_ACCEPT workaround above
             */
            length -= 4;

        if (buffer_info-&gt;rxbuf.data == NULL)
            skb_put(skb, length);
        else /* copybreak skb */
            skb_trim(skb, length);

        /* Receive Checksum Offload */
        e1000_rx_checksum(adapter,
                  (u32)(status) |
                  ((u32)(rx_desc-&gt;errors) &lt;&lt; 24),
                  le16_to_cpu(rx_desc-&gt;csum), skb);

        e1000_receive_skb(adapter, status, rx_desc-&gt;special, skb);

next_desc:
        rx_desc-&gt;status = 0;

        /* return some buffers to hardware, one at a time is too slow */
        if (unlikely(cleaned_count &gt;= E1000_RX_BUFFER_WRITE)) {
            adapter-&gt;alloc_rx_buf(adapter, rx_ring, cleaned_count);
            cleaned_count = 0;
        }

        /* use prefetched values */
        rx_desc = next_rxd;
        buffer_info = next_buffer;
    }
    rx_ring-&gt;next_to_clean = i;

    cleaned_count = E1000_DESC_UNUSED(rx_ring);
    if (cleaned_count)
        adapter-&gt;alloc_rx_buf(adapter, rx_ring, cleaned_count);

    adapter-&gt;total_rx_packets += total_rx_packets;
    adapter-&gt;total_rx_bytes += total_rx_bytes;
    netdev-&gt;stats.rx_bytes += total_rx_bytes;
    netdev-&gt;stats.rx_packets += total_rx_packets;
    return cleaned;
}
</code></pre>
<p>e1000_receive_skb实际调用napi_gro_receive将数据发送出去：</p>
<pre><code class="language-c">static void e1000_receive_skb(struct e1000_adapter *adapter, u8 status,
                  __le16 vlan, struct sk_buff *skb)
{
    skb-&gt;protocol = eth_type_trans(skb, adapter-&gt;netdev);

    if (status &amp; E1000_RXD_STAT_VP) {
        u16 vid = le16_to_cpu(vlan) &amp; E1000_RXD_SPC_VLAN_MASK;

        __vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);
    }
    napi_gro_receive(&amp;adapter-&gt;napi, skb);
}
</code></pre>
<p>函数napi_gro_receive首先会尝试通过GRO（Generic Receive
Offload)的方式将数据发送出去，如果网卡本身不支持GRO则会直接将数据报传送给上层协议栈（简单来说GRO就是将数据包累积到一定数量后再传给上层，这样一次性的处理多个数据包从而提升效率）</p>
<pre><code class="language-c">// kernel/net/dev.c
gro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)
{
    trace_napi_gro_receive_entry(skb);

    skb_gro_reset_offset(skb);

    return napi_skb_finish(dev_gro_receive(napi, skb), skb);
}
EXPORT_SYMBOL(napi_gro_receive);
</code></pre>
<p>intel这个网卡没有开启GRO，所以实际dev_gro_receive直接返回了GRO_NORMAL，这样就通过netif_receive_skb_internal处理数据包：</p>
<pre><code class="language-c">static gro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)
{
    switch (ret) {
    case GRO_NORMAL:
        if (netif_receive_skb_internal(skb))
            ret = GRO_DROP;
        break;

    case GRO_DROP:
        kfree_skb(skb);
        break;

    case GRO_MERGED_FREE:
        if (NAPI_GRO_CB(skb)-&gt;free == NAPI_GRO_FREE_STOLEN_HEAD)
            napi_skb_free_stolen_head(skb);
        else
            __kfree_skb(skb);
        break;

    case GRO_HELD:
    case GRO_MERGED:
        break;
    }

    return ret;
}
</code></pre>
<p>对于多核系统来说，一般数据传输处理的CPU跟中断处理的CPU是一致的，后来随着网卡速度的提升，如果把网卡的数据都放到一个CPU处理的话，会导致CPU负载过大进而导致数据传输的延迟，因此有人提出了RPS(Receive
packet steering,
就是将数据包的处理任务均衡的分配到各个CPU；要支持该特性，需要打开配置CONFIG_RPS，
同时在内核的配置中/sys/class/net/ethx/queues/rx-0/rps_cpus中将需要处理数据包的CPU设置为1，
这样在处理数据的时候就会将数据包先放到各个CPU的数据队列中进行处理。</p>
<p>这里假定该网卡没有配置RPS，接着会调用__netif_receive_skb处理网络数据。</p>
<pre><code class="language-c">static int netif_receive_skb_internal(struct sk_buff *skb)
{
    int ret;

    net_timestamp_check(netdev_tstamp_prequeue, skb);

    if (skb_defer_rx_timestamp(skb))
        return NET_RX_SUCCESS;

    rcu_read_lock();

#ifdef CONFIG_RPS
    if (static_key_false(&amp;rps_needed)) {
        struct rps_dev_flow voidflow, *rflow = &amp;voidflow;
        int cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow);

        if (cpu &gt;= 0) {
            ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail);
            rcu_read_unlock();
            return ret;
        }
    }
#endif
    ret = __netif_receive_skb(skb);
    rcu_read_unlock();
    return ret;
}
</code></pre>
<p>__netif_receive_skb实际调用__netif_receive_skb_core处理数据：__netif_receive_skb_core调用内核初始化时注册的协议类型，并调用其回调函数，由相应的协议来处理该数据包</p>
<pre><code class="language-c">static int __netif_receive_skb(struct sk_buff *skb)
{
    int ret;

    if (sk_memalloc_socks() &amp;&amp; skb_pfmemalloc(skb)) {
        unsigned long pflags = current-&gt;flags;

        /*
         * PFMEMALLOC skbs are special, they should
         * - be delivered to SOCK_MEMALLOC sockets only
         * - stay away from userspace
         * - have bounded memory usage
         *
         * Use PF_MEMALLOC as this saves us from propagating the allocation
         * context down to all allocation sites.
         */
        current-&gt;flags |= PF_MEMALLOC;
        ret = __netif_receive_skb_core(skb, true);
        tsk_restore_flags(current, pflags, PF_MEMALLOC);
    } else
        ret = __netif_receive_skb_core(skb, false);

    return ret;
}

static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)
{
    struct packet_type *ptype, *pt_prev;
    rx_handler_func_t *rx_handler;
    struct net_device *orig_dev;
    bool deliver_exact = false;
    int ret = NET_RX_DROP;
    __be16 type;

    net_timestamp_check(!netdev_tstamp_prequeue, skb);

    trace_netif_receive_skb(skb);

    orig_dev = skb-&gt;dev;

    skb_reset_network_header(skb);
    if (!skb_transport_header_was_set(skb))
        skb_reset_transport_header(skb);
    skb_reset_mac_len(skb);

    pt_prev = NULL;

another_round:
    skb-&gt;skb_iif = skb-&gt;dev-&gt;ifindex;

    __this_cpu_inc(softnet_data.processed);

    if (skb-&gt;protocol == cpu_to_be16(ETH_P_8021Q) ||
        skb-&gt;protocol == cpu_to_be16(ETH_P_8021AD)) {
        skb = skb_vlan_untag(skb);
        if (unlikely(!skb))
            goto out;
    }
    ...
    // 遍历已注册的协议，并调用其回调函数(一般是libpcap通过`AF_PACKET`传入的)
    list_for_each_entry_rcu(ptype, &amp;ptype_all, list) {
        if (pt_prev)
            ret = deliver_skb(skb, pt_prev, orig_dev);
        pt_prev = ptype;
    }

    // 协议栈
    list_for_each_entry_rcu(ptype, &amp;skb-&gt;dev-&gt;ptype_all, list) {
        if (pt_prev)
            ret = deliver_skb(skb, pt_prev, orig_dev);
        pt_prev = ptype;
    }

skip_taps:
#ifdef CONFIG_NET_INGRESS
    if (static_key_false(&amp;ingress_needed)) {
        skb = handle_ing(skb, &amp;pt_prev, &amp;ret, orig_dev);
        if (!skb)
            goto out;

        if (nf_ingress(skb, &amp;pt_prev, &amp;ret, orig_dev) &lt; 0)
            goto out;
    }
#endif
 ... 
}
</code></pre>
<p>回调协议包 struct packet_type注册的回调函数，把sk_buff传给该协议层处理。</p>
<pre><code class="language-c">static inline int deliver_skb(struct sk_buff *skb,
                  struct packet_type *pt_prev,
                  struct net_device *orig_dev)
{
    if (unlikely(skb_orphan_frags(skb, GFP_ATOMIC)))
        return -ENOMEM;
    atomic_inc(&amp;skb-&gt;users);
    return pt_prev-&gt;func(skb, skb-&gt;dev, pt_prev, orig_dev);
}
</code></pre>
<p>所有这些 struct
packet_type实际都是在内核初始化的时候通过dev_add_pack注册的，有兴趣的可以跟踪下对应的代码逻辑。至此,
数据包从设备物理层L1传输到了链路层L2了,传输完成了第一步.</p>
<p>总的说来, NAPI针对高负载的数据通讯做了优化, 减少了物理中断数量,
同时又兼顾了各个物理设备在传输调度上的公平性.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../Linux/index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../Go/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../Linux/index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../Go/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../editor.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
